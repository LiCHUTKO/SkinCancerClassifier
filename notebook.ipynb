{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie datasetu i znalezienie właściwych ścieżek\n",
    "print(\"Pobieranie datasetu HAM10000...\")\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "base_path = path[0]\n",
    "\n",
    "# Znajdowanie pliku metadata\n",
    "print(\"\\nSzukanie pliku metadata...\")\n",
    "metadata_file = None\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file == 'HAM10000_metadata.csv':\n",
    "            metadata_file = os.path.join(root, file)\n",
    "            print(f\"Znaleziono plik metadata: {metadata_file}\")\n",
    "            break\n",
    "    if metadata_file:\n",
    "        break\n",
    "\n",
    "# Wczytanie metadanych\n",
    "print(\"\\nWczytywanie metadanych...\")\n",
    "df = pd.read_csv(metadata_file)\n",
    "\n",
    "# Znajdowanie katalogów z obrazami\n",
    "print(\"\\nSzukanie obrazów...\")\n",
    "images_dirs = []\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name.startswith('HAM10000_images'):\n",
    "            images_dirs.append(os.path.join(root, dir_name))\n",
    "\n",
    "print(f\"Znalezione katalogi z obrazami: {images_dirs}\")\n",
    "\n",
    "# Przygotowanie ścieżek do obrazów\n",
    "def find_image_path(image_id):\n",
    "    for dir_path in images_dirs:\n",
    "        img_path = os.path.join(dir_path, f'{image_id}.jpg')\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "    return None\n",
    "\n",
    "df['filepath'] = df['image_id'].apply(find_image_path)\n",
    "\n",
    "# Usunięcie wierszy z brakującymi obrazami\n",
    "df = df.dropna(subset=['filepath'])\n",
    "\n",
    "# Wyświetl rozkład klas\n",
    "print(\"\\nRozkład klas w zbiorze:\")\n",
    "print(df['dx'].value_counts())\n",
    "\n",
    "# Podział na zbiory treningowy, walidacyjny i testowy (60-20-20)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, stratify=df['dx'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['dx'], random_state=42)\n",
    "\n",
    "print(f\"\\nRozmiary zbiorów:\")\n",
    "print(f\"Rozmiar zbioru treningowego: {len(train_df)}\")\n",
    "print(f\"Rozmiar zbioru walidacyjnego: {len(val_df)}\")\n",
    "print(f\"Rozmiar zbioru testowego: {len(test_df)}\")\n",
    "\n",
    "# Parametry\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Obliczenie wag klas (zachowujemy to dla lepszego uczenia)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['dx']),\n",
    "    y=train_df['dx']\n",
    ")\n",
    "class_weight_dict = dict(zip(range(len(np.unique(train_df['dx']))), class_weights))\n",
    "\n",
    "# Generator dla treningu z rozszerzoną augmentacją\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8,1.2],\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "# Generator dla walidacji i testu\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generatory danych\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filepath',\n",
    "    y_col='dx',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='filepath',\n",
    "    y_col='dx',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filepath',\n",
    "    y_col='dx',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Wyświetl rozkład klas w każdym zbiorze\n",
    "print(\"\\nRozkład klas w zbiorach:\")\n",
    "print(\"\\nZbiór treningowy:\")\n",
    "print(train_df['dx'].value_counts())\n",
    "print(\"\\nZbiór walidacyjny:\")\n",
    "print(val_df['dx'].value_counts())\n",
    "print(\"\\nZbiór testowy:\")\n",
    "print(test_df['dx'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przykładowe zdjecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Słownik z opisami klas\n",
    "dx_descriptions = {\n",
    "    'akiec': 'Rogowacenie słoneczne / Rak kolczystokomórkowy (in situ)',\n",
    "    'bcc': 'Rak podstawnokomórkowy',\n",
    "    'bkl': 'Łagodne zmiany keratotyczne (keratosis benigna)',\n",
    "    'df': 'Włókniak twardy (dermatofibroma)',\n",
    "    'nv': 'Znamię melanocytowe',\n",
    "    'mel': 'Czerniak',\n",
    "    'vasc': 'Zmiany naczyniowe'\n",
    "}\n",
    "\n",
    "# Pobierz po jednym przykładzie z każdej klasy\n",
    "examples = {}\n",
    "for dx in df['dx'].unique():\n",
    "    example = df[df['dx'] == dx].iloc[0]\n",
    "    examples[dx] = example['filepath']\n",
    "\n",
    "# Utwórz wykres\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for idx, (dx, img_path) in enumerate(examples.items(), 1):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(2, 4, idx)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{dx.upper()}\\n{dx_descriptions[dx]}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    except Exception as e:\n",
    "        print(f\"Nie udało się załadować obrazu dla klasy {dx}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wyświetl statystyki dla każdej klasy\n",
    "print(\"\\nLiczba przypadków w każdej klasie:\")\n",
    "class_counts = df['dx'].value_counts()\n",
    "for dx in df['dx'].unique():\n",
    "    print(f\"\\n{dx.upper()} - {dx_descriptions[dx]}\")\n",
    "    print(f\"Liczba przypadków: {class_counts[dx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"Trenowanie modelu CNN od podstaw...\")\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_cnn.summary()\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Zapisz model\n",
    "model_cnn.save('models/model_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja wyników dla modelu CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_cnn.history['loss'], label='Strata treningowa')\n",
    "plt.plot(history_cnn.history['val_loss'], label='Strata walidacyjna')\n",
    "plt.title('Loss - CNN')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Strata')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cnn.history['accuracy'], label='Dokładność treningowa')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='Dokładność walidacyjna')\n",
    "plt.title('Accuracy - CNN')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Dokładność')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macierz pomyłek dla modelu CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = model_cnn.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=test_generator.class_indices.keys())\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.show()\n",
    "\n",
    "# Procentowa macierz pomyłek\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "cmd_percentage = ConfusionMatrixDisplay(cm_percentage, display_labels=test_generator.class_indices.keys())\n",
    "cmd_percentage.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Percentage Confusion Matrix - CNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macierz pomyłek dla wczytanego modelu CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Wczytaj zapisany model\n",
    "model_cnn = load_model('models/model_cnn.h5')\n",
    "\n",
    "# Przewidywania dla zbioru testowego\n",
    "y_true = test_generator.classes\n",
    "y_pred = model_cnn.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=test_generator.class_indices.keys())\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.show()\n",
    "\n",
    "# Procentowa macierz pomyłek\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "cmd_percentage = ConfusionMatrixDisplay(cm_percentage, display_labels=test_generator.class_indices.keys())\n",
    "cmd_percentage.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Percentage Confusion Matrix - CNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Transfer Learning z MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trenowanie modelu z Transfer Learningiem (MobileNetV2)...\")\n",
    "\n",
    "# Wczytanie pretrenowanego modelu MobileNetV2 (bez głowy klasyfikacyjnej)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
    "base_model.trainable = False  # Zamrażamy warstwy MobileNetV2\n",
    "\n",
    "model_tl = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Redukcja wymiarów wyjścia z MobileNetV2\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model_tl.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_tl.summary()\n",
    "\n",
    "history_tl = model_tl.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // batch_size,\n",
    "    epochs=25\n",
    ")\n",
    "\n",
    "# Zapisz model\n",
    "model_tl.save('models/model_tl.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizualizacja wyników dla modelu z transfer learningiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_tl.history['loss'], label='Strata treningowa')\n",
    "plt.plot(history_tl.history['val_loss'], label='Strata walidacyjna')\n",
    "plt.title('Loss - Transfer Learning')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Strata')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_tl.history['accuracy'], label='Dokładność treningowa')\n",
    "plt.plot(history_tl.history['val_accuracy'], label='Dokładność walidacyjna')\n",
    "plt.title('Accuracy - Transfer Learning')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Dokładność')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macierz pomyłek dla modelu z transfer learningiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przewidywania dla zbioru testowego\n",
    "y_true_tl = test_generator.classes\n",
    "y_pred_tl = model_tl.predict(test_generator)\n",
    "y_pred_classes_tl = np.argmax(y_pred_tl, axis=1)\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm_tl = confusion_matrix(y_true_tl, y_pred_classes_tl)\n",
    "cmd_tl = ConfusionMatrixDisplay(cm_tl, display_labels=test_generator.class_indices.keys())\n",
    "cmd_tl.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Transfer Learning')\n",
    "plt.show()\n",
    "\n",
    "# Procentowa macierz pomyłek\n",
    "cm_percentage_tl = cm_tl.astype('float') / cm_tl.sum(axis=1)[:, np.newaxis] * 100\n",
    "cmd_percentage_tl = ConfusionMatrixDisplay(cm_percentage_tl, display_labels=test_generator.class_indices.keys())\n",
    "cmd_percentage_tl.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Percentage Confusion Matrix - Transfer Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macierz pomyłek dla wczytanego modelu z transfer learningiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj zapisany model\n",
    "model_tl = load_model('models/model_tl.h5')\n",
    "\n",
    "# Przewidywania dla zbioru testowego\n",
    "y_true_tl = test_generator.classes\n",
    "y_pred_tl = model_tl.predict(test_generator)\n",
    "y_pred_classes_tl = np.argmax(y_pred_tl, axis=1)\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm_tl = confusion_matrix(y_true_tl, y_pred_classes_tl)\n",
    "cmd_tl = ConfusionMatrixDisplay(cm_tl, display_labels=test_generator.class_indices.keys())\n",
    "cmd_tl.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Transfer Learning')\n",
    "plt.show()\n",
    "\n",
    "# Procentowa macierz pomyłek\n",
    "cm_percentage_tl = cm_tl.astype('float') / cm_tl.sum(axis=1)[:, np.newaxis] * 100\n",
    "cmd_percentage_tl = ConfusionMatrixDisplay(cm_percentage_tl, display_labels=test_generator.class_indices.keys())\n",
    "cmd_percentage_tl.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Percentage Confusion Matrix - Transfer Learning')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
